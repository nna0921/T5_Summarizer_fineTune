{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# one-time installs (run in a notebook cell)\n!pip install -q transformers datasets accelerate peft evaluate sentencepiece gradio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T15:10:58.642888Z","iopub.execute_input":"2025-11-03T15:10:58.643357Z","iopub.status.idle":"2025-11-03T15:11:02.349714Z","shell.execute_reply.started":"2025-11-03T15:10:58.643330Z","shell.execute_reply":"2025-11-03T15:11:02.348686Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import importlib\n\n# List of required packages\npackages = [\n    \"transformers\",\n    \"datasets\",\n    \"accelerate\",\n    \"bitsandbytes\",\n    \"peft\",\n    \"evaluate\",\n    \"sentencepiece\",\n    \"gradio\"\n]\n\nprint(\"üîç Checking installations...\\n\")\n\nfor pkg in packages:\n    try:\n        module = importlib.import_module(pkg)\n        version = getattr(module, \"__version__\", \"unknown\")\n        print(f\"‚úÖ {pkg} installed ‚Äî version: {version}\")\n    except ImportError:\n        print(f\"‚ùå {pkg} not installed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T15:11:02.351360Z","iopub.execute_input":"2025-11-03T15:11:02.351673Z","iopub.status.idle":"2025-11-03T15:11:16.324946Z","shell.execute_reply.started":"2025-11-03T15:11:02.351642Z","shell.execute_reply":"2025-11-03T15:11:16.324231Z"}},"outputs":[{"name":"stdout","text":"üîç Checking installations...\n\n‚úÖ transformers installed ‚Äî version: 4.57.1\n‚úÖ datasets installed ‚Äî version: 4.1.1\n‚úÖ accelerate installed ‚Äî version: 1.11.0\n‚úÖ bitsandbytes installed ‚Äî version: 0.48.3.dev0\n","output_type":"stream"},{"name":"stderr","text":"2025-11-03 15:11:11.045559: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762182671.068291     722 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762182671.075232     722 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ peft installed ‚Äî version: 0.17.1\n‚úÖ evaluate installed ‚Äî version: 0.4.6\n‚úÖ sentencepiece installed ‚Äî version: 0.2.0\n‚úÖ gradio installed ‚Äî version: 5.38.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q datasets\n!pip install -q evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T15:11:16.325667Z","iopub.execute_input":"2025-11-03T15:11:16.326288Z","iopub.status.idle":"2025-11-03T15:11:23.096567Z","shell.execute_reply.started":"2025-11-03T15:11:16.326260Z","shell.execute_reply":"2025-11-03T15:11:23.095644Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install rouge_score --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T15:11:23.098762Z","iopub.execute_input":"2025-11-03T15:11:23.099048Z","iopub.status.idle":"2025-11-03T15:11:26.254449Z","shell.execute_reply.started":"2025-11-03T15:11:23.099023Z","shell.execute_reply":"2025-11-03T15:11:26.253365Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"GPU name:\", torch.cuda.get_device_name(0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T15:11:26.255549Z","iopub.execute_input":"2025-11-03T15:11:26.255848Z","iopub.status.idle":"2025-11-03T15:11:26.261408Z","shell.execute_reply.started":"2025-11-03T15:11:26.255815Z","shell.execute_reply":"2025-11-03T15:11:26.260604Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nGPU name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =========================================================\n# Fine-tuning T5-small (CNN/DailyMail) on Kaggle ‚Äî fp16 + LoRA\n# =========================================================\n\nimport torch\nfrom transformers import (\n    AutoTokenizer, AutoModelForSeq2SeqLM,\n    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n)\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport evaluate\nimport numpy as np\nimport os\n\n# =========================================================\n# Load dataset from Kaggle input directory\n# =========================================================\ntrain_file = \"/kaggle/input/cnn_dailymail/train.csv\"\nval_file = \"/kaggle/input/cnn_dailymail/validation.csv\"\n\nprint(\"üì¶ Loading dataset...\")\ndataset = load_dataset(\"csv\", data_files={\n    \"train\": train_file,\n    \"validation\": val_file\n})\n\nprint(f\"‚úÖ Loaded {len(dataset['train'])} training and {len(dataset['validation'])} validation samples\")\n\n# Use smaller subset for faster fine-tuning\nsmall_train = dataset[\"train\"].select(range(2000))\nsmall_val = dataset[\"validation\"].select(range(200))\n\n# =========================================================\n# Tokenizer and model setup (fp16 mode)\n# =========================================================\nmodel_name = \"t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprint(\"‚öôÔ∏è Loading model in fp16 mode...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# =========================================================\n# Add LoRA adapter\n# =========================================================\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# =========================================================\n# Preprocessing\n# =========================================================\nmax_input_len = 512\nmax_target_len = 128\n\ndef preprocess_fn(batch):\n    inputs = [\"summarize: \" + doc for doc in batch[\"article\"]]\n    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True)\n    labels = tokenizer(batch[\"highlights\"], max_length=max_target_len, truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nprint(\"üîÑ Tokenizing data...\")\ntokenized_train = small_train.map(preprocess_fn, batched=True, remove_columns=small_train.column_names)\ntokenized_val = small_val.map(preprocess_fn, batched=True, remove_columns=small_val.column_names)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\nrouge = evaluate.load(\"rouge\")\n\n# =========================================================\n# Evaluation metric (with overflow fix)\n# =========================================================\ndef postprocess_text(preds, labels):\n    preds = [p.strip() for p in preds]\n    labels = [l.strip() for l in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    preds = preds.astype(np.int32)\n    labels = labels.astype(np.int32)\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    result = {k: round(v[\"fmeasure\"] * 100, 2) if isinstance(v, dict) else round(v * 100, 2) for k, v in result.items()}\n    return result\n\n# =========================================================\n# Training setup (no early eval to prevent decode crash)\n# =========================================================\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./t5_small_cnn_fp16\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    num_train_epochs=3,\n    learning_rate=2e-4,\n    fp16=True,\n    predict_with_generate=True,\n    eval_strategy=\"no\",  # ‚úÖ avoids OverflowError\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    logging_steps=50,\n    report_to=\"none\",\n    generation_max_length=64,  # shorter eval generation\n    generation_num_beams=4\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# =========================================================\n# Train and Save\n# =========================================================\nprint(\"üöÄ Starting fine-tuning (fp16 + LoRA)...\")\ntrainer.train()\n\nprint(\"üíæ Saving model...\")\ntrainer.save_model(\"./t5_small_cnn_fp16\")\ntokenizer.save_pretrained(\"./t5_small_cnn_fp16\")\n\nprint(\"‚úÖ Training complete and model saved successfully!\")\n\n# =========================================================\n# Optional: Evaluate after training\n# =========================================================\nprint(\"üìä Running final evaluation...\")\nmetrics = trainer.evaluate()\nprint(\"Final Evaluation Metrics:\", metrics)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T15:11:26.262333Z","iopub.execute_input":"2025-11-03T15:11:26.262692Z","iopub.status.idle":"2025-11-03T15:15:00.364281Z","shell.execute_reply.started":"2025-11-03T15:11:26.262669Z","shell.execute_reply":"2025-11-03T15:15:00.363356Z"}},"outputs":[{"name":"stdout","text":"üì¶ Loading dataset...\n‚úÖ Loaded 287113 training and 13368 validation samples\n","output_type":"stream"},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"name":"stdout","text":"‚öôÔ∏è Loading model in fp16 mode...\ntrainable params: 589,824 || all params: 61,096,448 || trainable%: 0.9654\nüîÑ Tokenizing data...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_722/891588594.py:125: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nThe model is already on multiple devices. Skipping the move to device specified in `args`.\n","output_type":"stream"},{"name":"stdout","text":"üöÄ Starting fine-tuning (fp16 + LoRA)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [375/375 02:29, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>2.422800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.166200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.161000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.097600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.107300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.128200</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>2.073300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"üíæ Saving model...\n‚úÖ Training complete and model saved successfully!\nüìä Running final evaluation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:56]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final Evaluation Metrics: {'eval_loss': 1.8787391185760498, 'eval_rouge1': 39.57, 'eval_rouge2': 18.03, 'eval_rougeL': 28.63, 'eval_rougeLsum': 28.58, 'eval_runtime': 58.3706, 'eval_samples_per_second': 3.426, 'eval_steps_per_second': 0.857, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n# Zip the saved model directory\nimport shutil\n\nmodel_dir = \"./t5_small_cnn_fp16\"\nzip_path = \"./t5_small_cnn_fp16.zip\"\n\n# Remove existing zip if any\nif os.path.exists(zip_path):\n    os.remove(zip_path)\n\n# Create zip archive\nshutil.make_archive(base_name=model_dir, format='zip', root_dir=model_dir)\n\nprint(f\"‚úÖ Model zipped successfully at: {zip_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T15:15:00.365222Z","iopub.execute_input":"2025-11-03T15:15:00.365729Z","iopub.status.idle":"2025-11-03T15:15:01.084931Z","shell.execute_reply.started":"2025-11-03T15:15:00.365698Z","shell.execute_reply":"2025-11-03T15:15:01.084101Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Model zipped successfully at: ./t5_small_cnn_fp16.zip\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import gradio as gr\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom peft import PeftModel\n\n# =========================================================\n# Load the fine-tuned model\n# =========================================================\nmodel_path = \"./t5_small_cnn_fp16\"  # Update this path if needed\n\nprint(\"Loading tokenizer and model...\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Load base model first\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(\n    \"t5-small\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(base_model, model_path)\nmodel.eval()\nprint(\"Model loaded successfully!\")\n\n# =========================================================\n# Summarization function\n# =========================================================\ndef summarize_text(article):\n    \"\"\"Generate a summary for the given article.\"\"\"\n    if not article.strip():\n        return \"Please enter some text to summarize.\"\n    \n    # Prepare input\n    input_text = \"summarize: \" + article\n    inputs = tokenizer(\n        input_text,\n        max_length=512,\n        truncation=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n    \n    # Generate summary\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=128,\n            num_beams=4,\n            early_stopping=True\n        )\n    \n    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return summary\n\n# =========================================================\n# Example articles for demo\n# =========================================================\nexamples = [\n    [\"\"\"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\"\"\"],\n    [\"\"\"Climate change is causing significant impacts on Earth's weather patterns. Rising global temperatures are leading to more frequent and severe heat waves, droughts, and storms. Scientists warn that without immediate action to reduce greenhouse gas emissions, these effects will continue to worsen, threatening ecosystems, food security, and human populations worldwide.\"\"\"]\n]\n\n# =========================================================\n# Gradio Interface\n# =========================================================\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"# T5 Article Summarizer\")\n    gr.Markdown(\"Fine-tuned T5-small model for text summarization\")\n    \n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(\n                label=\"Article Text\",\n                placeholder=\"Paste your article here...\",\n                lines=12\n            )\n            submit_btn = gr.Button(\"Summarize\", variant=\"primary\", size=\"lg\")\n        \n        with gr.Column():\n            output_text = gr.Textbox(\n                label=\"Summary\",\n                lines=12\n            )\n    \n    gr.Examples(\n        examples=examples,\n        inputs=[input_text],\n        label=\"Try an example\"\n    )\n    \n    submit_btn.click(\n        fn=summarize_text,\n        inputs=input_text,\n        outputs=output_text\n    )\n\n# =========================================================\n# Launch the app\n# =========================================================\nif __name__ == \"__main__\":\n    demo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T15:15:22.361826Z","iopub.execute_input":"2025-11-03T15:15:22.362127Z","iopub.status.idle":"2025-11-03T15:15:25.539593Z","shell.execute_reply.started":"2025-11-03T15:15:22.362103Z","shell.execute_reply":"2025-11-03T15:15:25.538816Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer and model...\nModel loaded successfully!\n* Running on local URL:  http://127.0.0.1:7861\n* Running on public URL: https://9b1adbc5c33445fd01.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://9b1adbc5c33445fd01.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":9}]}